{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02fce15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import psycopg2 as pg2\n",
    "import math\n",
    "from datetime import date, datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "828cab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondPCLAnalysis:\n",
    "    \n",
    "    def __init__(self, password):\n",
    "        self.password = password\n",
    "\n",
    "    def data_from_redshift(self, file_name):\n",
    "        #Getting the original data\n",
    "        print(\"[INFO] Establisging connection with AWS\")\n",
    "        conn = pg2.connect(dbname='edwreplica',\n",
    "                           user='rjangada@upgrade.com',\n",
    "                           password=self.password,\n",
    "                           host='edwreplica.ci0plcdfijlw.us-west-2.redshift.amazonaws.com')\n",
    "        print(\"[INFO] Connection successful\")\n",
    "        \n",
    "        query = (\"\"\"\n",
    "        With cte as(\n",
    "        select b.ssn_hash, lir.id, \n",
    "        convert_timezone('PST',lir.create_date::date)::date as create_date,\n",
    "        lir.product_type \n",
    "        from loanreview.loan_in_review as lir \n",
    "        left join funnel.loan_application as lap\n",
    "        on lir.id = lap.id\n",
    "        left join funnel.borrower as b\n",
    "        on lap.borrower_id = b.id\n",
    "        where lir.loan_status in('OPENED','ISSUED') \n",
    "        order by b.ssn_hash, convert_timezone('PST',lir.create_date::date)::date)\n",
    "\n",
    "        select * ssn_hash, id, create_date, product_type\n",
    "        from cte\n",
    "                \"\"\"\n",
    "                )\n",
    "        print(\"[INFO] Running SQL query\")\n",
    "        raw_data = pd.read_sql_query(sql=query, con=conn)\n",
    "        print(\"[INFO] Data received from Redshift\")\n",
    "        raw_data.to_csv(file_name)\n",
    "        conn.close()\n",
    "        return\n",
    "    \n",
    "    def file_splitter(self, csv_file):\n",
    "\n",
    "        total_rows = sum(1 for row in (open(csv_file)))\n",
    "        batch_size = 1000000\n",
    "        file_list = []\n",
    "        print(\"[INFO] Splitting the data from Redshift\")\n",
    "        \n",
    "        \n",
    "        df = pd.read_csv(csv_file)\n",
    "        columns = list(df.head(0))\n",
    "        #columns=columns[1:]\n",
    "        \n",
    "\n",
    "        for i in range(1, total_rows, batch_size):\n",
    "            df = pd.read_csv(csv_file, header=0, nrows = batch_size, skiprows = i)\n",
    "            df.columns=columns\n",
    "            file_name = 'input' + str(i) + '.csv'\n",
    "            file_list.append(file_name)\n",
    "            print(\"[INFO] Creating file: \" + file_name)\n",
    "            df.to_csv(file_name, index=False, header=columns, mode='a', chunksize=batch_size)\n",
    "        return file_list\n",
    "    \n",
    "    def get_subset_data(self, df, ssn_hash, index, n):\n",
    "        start = end = index\n",
    "        while(start >= 0 and df.iloc[start]['ssn_hash'] == ssn_hash):\n",
    "            start = start - 1\n",
    "\n",
    "\n",
    "        while(end < n and df.iloc[end]['ssn_hash'] == ssn_hash):\n",
    "            end = end + 1\n",
    "\n",
    "        return start + 1, end\n",
    "    \n",
    "    def process_data(self, file_list):\n",
    "        file_output = []\n",
    "        for i in range(len(file_list)):\n",
    "            file_output.append(self.product_purchase_analysis(file_list[i], \"output\" + str(i) + \".csv\"))\n",
    "        return file_output\n",
    "    \n",
    "    def product_purchase_analysis(self, file_to_read, file_to_write):\n",
    "        print(\"[INFO] Doing product_purchase_analysis for: \" + file_to_read)\n",
    "        \n",
    "        df = pd.read_csv(file_to_read)\n",
    "        df = df.drop('Unnamed: 0', axis=1)\n",
    "        n = len(df)\n",
    "        df['Date'] = pd.to_datetime(df['create_date']).dt.date\n",
    "        print(\"[INFO] Length of data: \" + str(n))\n",
    "        \n",
    "\n",
    "        date=datetime.datetime.now().date()\n",
    "        for i in range(n):\n",
    "            start, end = self.get_subset_data(df, df.iloc[i]['ssn_hash'], i, n)\n",
    "            subset_data = df[start:end]\n",
    "\n",
    "\n",
    "            date = df.iloc[i]['Date']\n",
    "            date_next3months = df.iloc[i]['Date'] + relativedelta(months=+3)\n",
    "            date_next6months = df.iloc[i]['Date'] + relativedelta(months=+6)\n",
    "            date_next12months = df.iloc[i]['Date'] + relativedelta(months=+12)\n",
    "            datetime_today = str(datetime.datetime.now().date())\n",
    "            date_today = pd.to_datetime(datetime_today)\n",
    "\n",
    "            count_orig = 0\n",
    "            count_pcl_orig = 0\n",
    "            count_pl_orig = 0\n",
    "            count_dep_orig = 0\n",
    "            count_hm_orig = 0\n",
    "            count_3 = 0\n",
    "            count_pcl_3 = 0\n",
    "            count_pl_3 = 0\n",
    "            count_dep_3 = 0\n",
    "            count_hm_3 = 0\n",
    "            count_6 = 0\n",
    "            count_pcl_6 = 0\n",
    "            count_pl_6 = 0\n",
    "            count_dep_6 = 0\n",
    "            count_hm_6 = 0\n",
    "            count_12 = 0\n",
    "            count_pcl_12 = 0\n",
    "            count_pl_12 = 0\n",
    "            count_dep_12 = 0\n",
    "            count_hm_12 = 0\n",
    "            count_today = 0\n",
    "            count_pcl_today = 0\n",
    "            count_pl_today = 0\n",
    "            count_dep_today = 0\n",
    "            count_hm_today = 0\n",
    "\n",
    "            if start != i:\n",
    "                subset_df_2 = df[start:i]\n",
    "                for j in range(len(subset_df_2)):\n",
    "                    if subset_df_2.iloc[j]['product_type'] == 'PERSONAL_CREDIT_LINE':\n",
    "                        count_pcl_orig = count_pcl_orig + 1\n",
    "                    elif subset_df_2.iloc[j]['product_type'] == 'PERSONAL_LOAN':\n",
    "                        count_pl_orig = count_pl_orig + 1\n",
    "                    elif subset_df_2.iloc[j]['product_type'] == 'DEPOSIT':\n",
    "                        count_dep_orig = count_dep_orig + 1\n",
    "                    elif subset_df_2.iloc[j]['product_type'] == 'HOME_IMPROVEMENT_CREDIT_LINE':\n",
    "                        count_hm_orig = count_hm_orig + 1\n",
    "                    else:\n",
    "                        0\n",
    "\n",
    "                count_orig = count_pcl_orig + count_pl_orig + count_dep_orig + count_hm_orig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            for j in range(len(subset_data)):\n",
    "                if date_next3months > subset_data.iloc[j]['Date']:\n",
    "                    count_3 = count_3 + 1\n",
    "                    if subset_data.iloc[j]['product_type'] == 'PERSONAL_CREDIT_LINE':\n",
    "                        count_pcl_3 = count_pcl_3 + 1\n",
    "                    elif subset_data.iloc[j]['product_type'] == 'PERSONAL_LOAN':\n",
    "                        count_pl_3 = count_pl_3 + 1\n",
    "                    elif subset_data.iloc[j]['product_type'] == 'DEPOSIT':\n",
    "                        count_dep_3 = count_dep_3 + 1\n",
    "                    elif subset_data.iloc[j]['product_type'] == 'HOME_IMPROVEMENT_CREDIT_LINE':\n",
    "                        count_hm_3 = count_hm_3 + 1\n",
    "                    else:\n",
    "                        0\n",
    "                if date_next6months > subset_data.iloc[j]['Date']:\n",
    "                    count_6 = count_6 + 1\n",
    "                    if subset_data.iloc[j]['product_type'] == 'PERSONAL_CREDIT_LINE':\n",
    "                        count_pcl_6 = count_pcl_6 + 1\n",
    "                    elif subset_data.iloc[j]['product_type'] == 'PERSONAL_LOAN':\n",
    "                        count_pl_6 = count_pl_6 + 1\n",
    "                    elif subset_data.iloc[j]['product_type'] == 'DEPOSIT':\n",
    "                        count_dep_6 = count_dep_6 + 1\n",
    "                    elif subset_data.iloc[j]['product_type'] == 'HOME_IMPROVEMENT_CREDIT_LINE':\n",
    "                        count_hm_6 = count_hm_6 + 1\n",
    "                    else:\n",
    "                        0\n",
    "\n",
    "                if date_next12months > subset_data.iloc[j]['Date']:\n",
    "                    count_12 = count_12 + 1\n",
    "                    if subset_data.iloc[j]['product_type'] == 'PERSONAL_CREDIT_LINE':\n",
    "                        count_pcl_12 = count_pcl_12 + 1\n",
    "                    elif subset_data.iloc[j]['product_type'] == 'PERSONAL_LOAN':\n",
    "                        count_pl_12 = count_pl_12 + 1\n",
    "                    elif subset_data.iloc[j]['product_type'] == 'DEPOSIT':\n",
    "                        count_dep_12 = count_dep_12 + 1\n",
    "                    elif subset_data.iloc[j]['product_type'] == 'HOME_IMPROVEMENT_CREDIT_LINE':\n",
    "                        count_hm_12 = count_hm_12 + 1\n",
    "                    else:\n",
    "                        0\n",
    "\n",
    "                if datetime_today > subset_data.iloc[j]['create_date']:\n",
    "                    count_today = count_today + 1\n",
    "                    if subset_data.iloc[j]['product_type'] == 'PERSONAL_CREDIT_LINE':\n",
    "                        count_pcl_today = count_pcl_today + 1\n",
    "                    elif subset_data.iloc[j]['product_type'] == 'PERSONAL_LOAN':\n",
    "                        count_pl_today = count_pl_today + 1\n",
    "                    elif subset_data.iloc[j]['product_type'] == 'DEPOSIT':\n",
    "                        count_dep_today = count_dep_today + 1\n",
    "                    elif subset_data.iloc[j]['product_type'] == 'HOME_IMPROVEMENT_CREDIT_LINE':\n",
    "                        count_hm_today = count_hm_today + 1\n",
    "                    else:\n",
    "                        0\n",
    "\n",
    "\n",
    "            df.at[i, 'cnt_tot_post_accts1_orig'] = count_orig\n",
    "            df.at[i, 'cnt_pcl_post_accts1_orig'] = count_pcl_orig\n",
    "            df.at[i, 'cnt_pl_post_accts1_orig'] = count_pl_orig\n",
    "            df.at[i, 'cnt_dep_post_accts1_orig'] = count_dep_orig\n",
    "            df.at[i, 'cnt_hm_post_accts1_orig'] = count_hm_orig\n",
    "            df.at[i, 'cnt_tot_post_accts1_3month'] = count_3\n",
    "            df.at[i, 'cnt_pcl_post_accts1_3month'] = count_pcl_3\n",
    "            df.at[i, 'cnt_pl_post_accts1_3month'] = count_pl_3\n",
    "            df.at[i, 'cnt_dep_post_accts1_3month'] = count_dep_3\n",
    "            df.at[i, 'cnt_hm_post_accts1_3month'] = count_hm_3\n",
    "            df.at[i, 'cnt_tot_post_accts1_6month'] = count_6\n",
    "            df.at[i, 'cnt_pcl_post_accts1_6month'] = count_pcl_6\n",
    "            df.at[i, 'cnt_pl_post_accts1_6month'] = count_pl_6\n",
    "            df.at[i, 'cnt_dep_post_accts1_6month'] = count_dep_6\n",
    "            df.at[i, 'cnt_hm_post_accts1_6month'] = count_hm_6\n",
    "            df.at[i, 'cnt_tot_post_accts1_12month'] = count_12\n",
    "            df.at[i, 'cnt_pcl_post_accts1_12month'] = count_pcl_12\n",
    "            df.at[i, 'cnt_pl_post_accts1_12month'] = count_pl_12\n",
    "            df.at[i, 'cnt_dep_post_accts1_12month'] = count_dep_12\n",
    "            df.at[i, 'cnt_hm_post_accts1_12month'] = count_hm_12\n",
    "            df.at[i, 'cnt_tot_post_accts1_today'] = count_today\n",
    "            df.at[i, 'cnt_pcl_post_accts1_today'] = count_pcl_today\n",
    "            df.at[i, 'cnt_pl_post_accts1_today'] = count_pl_today\n",
    "            df.at[i, 'cnt_dep_post_accts1_today'] = count_dep_today\n",
    "            df.at[i, 'cnt_hm_post_accts1_today'] = count_hm_today\n",
    "        df = df.drop('create_date', axis=1)\n",
    "        df.to_csv(file_to_write)\n",
    "        return file_to_write\n",
    "    \n",
    "    \n",
    "    def merge_output(self, file_list, file_output):\n",
    "        print(\"[INFO] Merging the outputs\")\n",
    "        df_list = []\n",
    "        for i in range(len(file_list)):\n",
    "            df = pd.read_csv(file_list[i])\n",
    "            #df['create_date'] =  pd.to_datetime(df['create_date'], infer_datetime_format=True)\n",
    "            df_list.append(df)\n",
    "            \n",
    "        df_big = pd.concat(df_list, ignore_index=True)\n",
    "        df_big = df_big.drop(['Unnamed: 0'], axis=1)\n",
    "        \n",
    "        df_big.to_csv(\"output_all_products.csv\")\n",
    "        \n",
    "        df_complete = df_big.loc[df_big['product_type'] == 'PERSONAL_CREDIT_LINE'] \n",
    "        df_complete.to_csv(file_output)\n",
    "        print(\"[INFO] Merge complete\")\n",
    "        print(\"[INFO] Created final output file: \" + file_output)\n",
    "        return\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "748b18b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter redshift passwordCgblV6kzr8AmQr4oF7gVCX0N\n",
      "[INFO] Establisging connection with AWS\n",
      "[INFO] Connection successful\n",
      "[INFO] Running SQL query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rjangada\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Data received from Redshift\n",
      "[INFO] Splitting the data from Redshift\n",
      "[INFO] Creating file: input1.csv\n",
      "[INFO] Creating file: input6.csv\n",
      "[INFO] Creating file: input11.csv\n",
      "[INFO] Doing product_purchase_analysis for: input1.csv\n",
      "[INFO] Length of data: 5\n",
      "[INFO] Doing product_purchase_analysis for: input6.csv\n",
      "[INFO] Length of data: 5\n",
      "[INFO] Doing product_purchase_analysis for: input11.csv\n",
      "[INFO] Length of data: 4\n",
      "[INFO] Merging the outputs\n",
      "[INFO] Merge complete\n",
      "[INFO] Created final output file: output_pcl.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    password=input(\"enter redshift password\")\n",
    "    redshift_file = 'redshift_data.csv'\n",
    "    secondPCLAnalysis = SecondPCLAnalysis(password)\n",
    "    secondPCLAnalysis.data_from_redshift(redshift_file)\n",
    "    file_list = secondPCLAnalysis.file_splitter(redshift_file)\n",
    "    file_output_list = secondPCLAnalysis.process_data(file_list)\n",
    "    secondPCLAnalysis.merge_output(file_output_list, \"output_pcl.csv\")\n",
    "\n",
    "main()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc9f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0045a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
